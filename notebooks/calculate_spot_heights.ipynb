{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculate spot heights notebook estimates a height above nearest neighbour by interogating the outputs of a hydraulic model at points upstream and downstream of a reach.\n",
    "\n",
    "Graphs of the water levels at the points are generated as a biproduct of this process\n",
    "\n",
    "It produces a file hand-levels.csv with the new levels. If you already have appropriate hand levels, then this step is not necessary (and will overwrite the values you already have)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import logging\n",
    "import cmcrameri.cm as cmc\n",
    "import numpy\n",
    "\n",
    "logging.getLogger().setLevel('WARNING')\n",
    "output_folder = os.environ['OUTPUT_FOLDER']\n",
    "hand_points_shape = os.environ['HAND_POINTS']\n",
    "zone_definition_albers_shape = os.environ['ZONE_DEF_ALBERS']\n",
    "highres = os.environ['HIGHRES'] == 'TRUE'\n",
    "if highres:\n",
    "    dpi=600\n",
    "    hextent=\"_high.tif\"\n",
    "    fig_args = {'format': \"tiff\", 'pil_kwargs': {\"compression\": \"tiff_lzw\"}}\n",
    "else:\n",
    "    dpi=300\n",
    "    hextent=\".png\"\n",
    "    fig_args = {'format':\"png\"}\n",
    "\n",
    "\n",
    "#colour_scheme = cmc.batlowS.colors\n",
    "colour_scheme = numpy.array([[0x1F, 0x77, 0xB4], [0xF2, 0x71, 0x07], [\n",
    "                            0xAA, 0xE0, 0x07], [0x8C, 0x56, 0x4B], [0xDB, 0x72, 0xBC]])/256.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Ignore \"ERROR:root:Hand level was not found for date 2016-08-18 in Murray - GKPF - 2016-07\" when run the first time (or update hand point file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydrological_connectivity.definitions.definitions_generator_factory import DefinitionsGeneratorFactory\n",
    "from rasterio.windows import from_bounds\n",
    "import rasterio\n",
    "from hydrological_connectivity.processing.compare_flood_rasters_rasterio import CompareFloodRastersRasterIo\n",
    "from hydrological_connectivity.processing.compare_flood_rasters_elev_rasterio import CompareFloodRastersElevRasterIo\n",
    "from hydrological_connectivity.processing.calculate_hydraulic_level import CalculateHydraulicLevel\n",
    "import logging\n",
    "import geopandas\n",
    "logging.getLogger().setLevel('INFO')\n",
    "definition = DefinitionsGeneratorFactory.get_generator()\n",
    "\n",
    "definition.generate()\n",
    "\n",
    "rows_points = geopandas.read_file(hand_points_shape).transpose()\n",
    "\n",
    "rows_regions = geopandas.read_file(zone_definition_albers_shape).transpose()\n",
    "\n",
    "depth_results = {}\n",
    "se_results = {}\n",
    "for segment_index in rows_points:\n",
    "    row_pt = rows_points[segment_index]\n",
    "    segment_id = row_pt.loc['Segment_Id']\n",
    "    description = row_pt.loc['Descript']\n",
    "    geometry = row_pt.loc['geometry']\n",
    "    if segment_id not in rows_regions:\n",
    "        logging.warning(f\"Hand point {description} skipped\")\n",
    "        continue\n",
    "    row_region = rows_regions[segment_id]\n",
    "\n",
    "    short_location_name = row_region['Short_Loc']\n",
    "    region = row_region.loc['Region']\n",
    "    logging.info(f\"processing {short_location_name}\")\n",
    "    depth_segment_results = {}\n",
    "    se_segment_results = {}\n",
    "\n",
    "    if not(region in definition.models):\n",
    "        logging.warning(\n",
    "            f\"Region '{region}' was not found in the list of models, SKIPPING\")\n",
    "        continue\n",
    "\n",
    "    hydraulic_model = definition.models[region]\n",
    "\n",
    "    contiguous_ranges = hydraulic_model.get_contiguous_date_ranges()\n",
    "\n",
    "    geometry_points = [(geometry.x, geometry.y)]\n",
    "    display(geometry_points)\n",
    "    for (date_rec, depth_raster_location) in hydraulic_model.depth_outputs.items():\n",
    "        range_index = 0\n",
    "        for contiguous in contiguous_ranges:\n",
    "            if (date_rec >= contiguous['start'] and date_rec <= contiguous['end']):\n",
    "                break\n",
    "            range_index = range_index+1\n",
    "\n",
    "        if range_index not in depth_segment_results:\n",
    "            depth_segment_results[range_index] = {}\n",
    "            se_segment_results[range_index] = {}\n",
    "\n",
    "        range_results = depth_segment_results[range_index]\n",
    "        se_range_results = se_segment_results[range_index]\n",
    "\n",
    "        surface_elevation_raster = hydraulic_model.elevation_outputs[date_rec]\n",
    "        elevation_raster = definition.global_elevation.elevation_filename\n",
    "        logging.info(f\"Elevation raster {elevation_raster}\")\n",
    "\n",
    "        if range_index == 0 and description == \"US SA - Weir Pool 5\":\n",
    "            offset_geometry = [(geometry.x+155/110000, geometry.y)]\n",
    "            logging.info(\n",
    "                f\"Special case {elevation_raster} {offset_geometry}\")\n",
    "            surface_elevation = CalculateHydraulicLevel(\n",
    "                surface_elevation_raster, offset_geometry)\n",
    "        else:\n",
    "            surface_elevation = CalculateHydraulicLevel(\n",
    "                surface_elevation_raster, geometry_points)\n",
    "\n",
    "        dem_elevation = CalculateHydraulicLevel(\n",
    "            elevation_raster, geometry_points)\n",
    "        depth = CalculateHydraulicLevel(depth_raster_location, geometry_points)\n",
    "\n",
    "        try:\n",
    "            surface_elevation.execute()\n",
    "            dem_elevation.execute()\n",
    "            depth.execute()\n",
    "            heights = depth.heights\n",
    "            if len(depth.heights) > 0:\n",
    "                if len(depth.heights[0]) > 0:\n",
    "                    heights = depth.heights[0][0]\n",
    "\n",
    "            surface_elevation_heights = surface_elevation.heights\n",
    "            if len(surface_elevation.heights) > 0:\n",
    "                if len(surface_elevation.heights[0]) > 0:\n",
    "                    surface_elevation_heights = surface_elevation.heights[0][0]\n",
    "\n",
    "            dem_elevation_heights = dem_elevation.heights\n",
    "            if len(dem_elevation.heights) > 0:\n",
    "                if len(dem_elevation.heights[0]) > 0:\n",
    "                    dem_elevation_heights = dem_elevation.heights[0][0]\n",
    "\n",
    "            logging.info(\n",
    "                f\"observed level of {heights}m for zone {segment_index} for file {depth_raster_location} - using elevation {surface_elevation_heights} vs {dem_elevation_heights}m\")\n",
    "            se_range_results[(date_rec,\n",
    "                            surface_elevation_raster)] = surface_elevation_heights - dem_elevation_heights\n",
    "            range_results[(date_rec,\n",
    "                        depth_raster_location)] = heights\n",
    "        except:\n",
    "            logging.exception(\n",
    "                f\"Problem with one or more of these files: {(depth_raster_location,surface_elevation_raster,elevation_raster)}\")\n",
    "    depth_results[segment_index] = depth_segment_results\n",
    "    se_results[segment_index] = se_segment_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = se_results\n",
    "#results = depth_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "Path(output_folder ).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "output = open(output_folder + f'{os.path.sep}hand_levels.pkl', 'wb')\n",
    "pickle.dump(results, output)\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "input_file = open(f'{output_folder}{os.path.sep}hand_levels.pkl', 'rb')\n",
    "results = pickle.load(input_file)\n",
    "input_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "flattened = []\n",
    "\n",
    "rows_points = geopandas.read_file(hand_points_shape).transpose()\n",
    "\n",
    "rows_regions = geopandas.read_file(zone_definition_albers_shape).transpose()\n",
    "\n",
    "for (zone, internals) in results.items():\n",
    "    for (contiguous_range, result_sec) in internals.items():\n",
    "        dates = []\n",
    "        volumes = []\n",
    "        for ((date_t, file_t), volume) in result_sec.items():\n",
    "            dates.append(date_t)\n",
    "            if (volume == numpy.finfo(numpy.float64).min):\n",
    "                volumes.append(numpy.nan)\n",
    "            else:\n",
    "                volumes.append(volume)\n",
    "\n",
    "        first_date = numpy.min(dates)\n",
    "\n",
    "        dataframe = (pandas.DataFrame(volumes, index=dates, columns=[\n",
    "                     f'{rows_points[zone][\"Descript\"]} {first_date:%Y-%m}']))\n",
    "        flattened.append(dataframe)\n",
    "\n",
    "result_set = pandas.concat(flattened, join='outer', axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "peak_event_df = pandas.read_csv(\n",
    "    f\"{os.environ['INPUT_FOLDER']}{os.path.sep}peak-events.csv\", index_col=0, parse_dates=[1])\n",
    "peak_events = peak_event_df.to_dict()['PeakEvent']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "x = {long_label: long_label[3:] for long_label in result_set.columns}\n",
    "swapped = {}\n",
    "for k, v in x.items():\n",
    "    if v not in swapped:\n",
    "        swapped[v] = set()\n",
    "    swapped[v].add(k)\n",
    "\n",
    "merged_series = {short_label: result_set[set_of_series].mean(axis=1, skipna=True) for short_label,\n",
    "                 set_of_series in swapped.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "import seaborn\n",
    "import logging\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "#seaborn.set_theme(style=\"whitegrid\")\n",
    "seaborn.set(font_scale=0.75)\n",
    "pyplot.style.use('seaborn-white')\n",
    "\n",
    "fig, axes_list = pyplot.subplots(\n",
    "    nrows=7, ncols=4, figsize=((8.3), (11.7-2)), sharey='row', dpi=dpi, gridspec_kw={'left': 0.045, 'right': 0.99, 'top': 0.97, 'bottom': 0.02, 'wspace': 0.05, 'hspace': 0.2})  # gridspec_kw={'left': 0.01, 'right': 1, 'top': 1, 'bottom': 0, 'wspace': 0.05, 'hspace': 0.05})\n",
    "\n",
    "fig.delaxes(ax=axes_list[6,2])\n",
    "fig.delaxes(ax=axes_list[6,3])\n",
    "\n",
    "grid_found=[]\n",
    "\n",
    "filtered_result_set = pandas.DataFrame(\n",
    "    result_set[[x for x in result_set.columns if '1956' not in x]])\n",
    "\n",
    "display_order = [22, 23, 24, 25, 0, 1, 2, 3, 4, 5, 6, 7,\n",
    "                 8, 9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 16, 17, ]\n",
    "\n",
    "# Ugly code to get the row-wise maximumum\n",
    "row_max_list = {j: [] for j in range(7)}\n",
    "for index in range(0, len(filtered_result_set.columns)):\n",
    "    i = index\n",
    "\n",
    "    col_lookup = filtered_result_set.columns[i][3:]\n",
    "    original = True\n",
    "    if col_lookup not in grid_found:\n",
    "        grid_found.append(col_lookup)\n",
    "        original = False\n",
    "\n",
    "    g_index = grid_found.index(col_lookup)\n",
    "    d_index = display_order.index(g_index)\n",
    "    result_array = filtered_result_set[filtered_result_set.columns[i]].loc[~numpy.isnan(\n",
    "        filtered_result_set[filtered_result_set.columns[i]])]\n",
    "    row_max_list[d_index//4].append(numpy.max(result_array))\n",
    "    #print((index, g_index, d_index, col_lookup, numpy.max(result_array)))\n",
    "\n",
    "row_max = {j: numpy.max(v, initial=0) for j, v in row_max_list.items()}\n",
    "\n",
    "for index in range(0, len(filtered_result_set.columns)):\n",
    "    i = index\n",
    "\n",
    "    col_lookup = filtered_result_set.columns[i][3:]\n",
    "    original=True\n",
    "    if col_lookup not in grid_found:\n",
    "        grid_found.append(col_lookup)\n",
    "        original=False\n",
    "\n",
    "    g_index = grid_found.index(col_lookup)\n",
    "    d_index = display_order.index(g_index)\n",
    "    axis = axes_list[d_index // 4][d_index % 4]\n",
    "    #display(f'{col_lookup}: {g_index}/{d_index}: ({d_index // 4},{d_index % 4})')\n",
    "    \n",
    "    result_array = filtered_result_set[filtered_result_set.columns[i]].loc[~numpy.isnan(\n",
    "        filtered_result_set[filtered_result_set.columns[i]])]\n",
    "\n",
    "    prod_desc = filtered_result_set.columns[i]\n",
    "\n",
    "    short_label = prod_desc[3:]\n",
    "    mean_series = merged_series[short_label]\n",
    "    graphical_max = row_max[d_index // 4]\n",
    "    seaborn.set_palette(colour_scheme)\n",
    "    if len(result_array) > 1:\n",
    "        axis.set_prop_cycle(color=[colour_scheme[0]] if f\"{filtered_result_set.columns[i][0:2]}\"=='US' else [colour_scheme[1]])\n",
    "        max_value = numpy.max(numpy.max(filtered_result_set[[x for x in numpy.max(\n",
    "            filtered_result_set).index if x[3:] == col_lookup]]))\n",
    "        max_index = numpy.argmax(\n",
    "            filtered_result_set[filtered_result_set.columns[i]])\n",
    "        max_value = graphical_max\n",
    "        res = seaborn.lineplot(x=\"index\", y=filtered_result_set.columns[i], \n",
    "        data=filtered_result_set.reset_index(),\n",
    "                               ax=axis)\n",
    "        if col_lookup in peak_events:\n",
    "            axis.vlines(peak_events[col_lookup], ymin=0,\n",
    "                        ymax=max_value*1.1, linewidth=1., color=colour_scheme[2])\n",
    "            logging.debug(\n",
    "                f\"Found '{filtered_result_set.columns[i]}' with value {peak_events[col_lookup]}\")\n",
    "        else:\n",
    "            logging.error(\n",
    "                f\"Did not find '{col_lookup}' derived from '{filtered_result_set.columns[i]}'\")\n",
    "\n",
    "        axis.vlines(filtered_result_set.index[max_index], ymin=0,\n",
    "                    ymax=max_value*1.1, linewidth=1., color='grey', linestyles='dashed')\n",
    "\n",
    "        axis.set_ylim(ymin=0)\n",
    "        res.set_ylabel(None)\n",
    "        res.set_xlabel(None)\n",
    "        res.set_xticklabels([])\n",
    "        #locator = mdates.AutoDateLocator(interval_multiples=False)\n",
    "        #axis.xaxis.set_major_locator(locator)\n",
    "        #pyplot.setp(axis.get_xticklabels(), rotation=45, ha=\"right\")   # optional\n",
    "        res.set_title(short_label)\n",
    "    elif len(result_array) == 1:\n",
    "        #data_frame = pandas.DataFrame.from_dict({'x': [f\"{ft:%Y-%m-%d}\" for ft in result_array.index], 'y': result_array.values,\n",
    "        #                                         'hue': [f\"{filtered_result_set.columns[i][0:2]}\"]})\n",
    "        #res = seaborn.barplot(data=data_frame, ax=axis)\n",
    "        res = seaborn.barplot(x=[f\"{ft:%Y-%m-%d}\" for ft in result_array.index], y=result_array.values,\n",
    "                              hue=[f\"{filtered_result_set.columns[i][0:2]}\"], hue_order=['US', 'DS'],\n",
    "                              ax=axis, palette=colour_scheme)\n",
    "\n",
    "        j=0\n",
    "        us_ds_label = {0: 'U/S', 1: 'D/S', 2: 'U/S', 3: 'D/S'}\n",
    "        for p in res.patches:\n",
    "            res.annotate(us_ds_label[j], (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                           ha='center', va='center', fontsize=6, color='black', xytext=(0, -10),\n",
    "                           textcoords='offset points')\n",
    "            j=j+1\n",
    "\n",
    "        res._remove_legend(res.get_legend())\n",
    "\n",
    "        res.set_title(short_label)\n",
    "        res.set_ylabel(None)\n",
    "        res.set_xlabel(None)\n",
    "        res.set_xticklabels([])\n",
    "\n",
    "        if col_lookup in peak_events:\n",
    "            chosen_date = peak_events[col_lookup]\n",
    "            level = mean_series.loc[chosen_date]\n",
    "        else:\n",
    "            chosen_date = mean_series.index[mean_series.argmax()]\n",
    "            level = mean_series.loc[chosen_date]\n",
    "    \n",
    "        #axis.text(f\"{chosen_date:%Y-%m-%d}\", level, f\"{level:02}\")\n",
    "\n",
    "fig.suptitle(\"Water level (m)\",\n",
    "             x=0.01, y=0.5, ha='center', va='center', rotation=90)\n",
    "\n",
    "fig.text(0.5, 0.01, \"Date range as per model\", ha='center', va='center')\n",
    "#fig.subplots_adjust(left=0.5)\n",
    "#fig.autofmt_xdate()\n",
    "#pyplot.tight_layout(rect=[0, 0, 1, 1])\n",
    "volume_file_name = output_folder + f\"{os.path.sep}fig1_supplement_hand_points\" + hextent\n",
    "pyplot.savefig(volume_file_name, **fig_args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta \n",
    "hand_params=[]\n",
    "for short_label, mean_series in merged_series.items():\n",
    "    for segment_index in rows_regions:\n",
    "        row_region = rows_regions[segment_index]\n",
    "        short_location_name = row_region['Short_Loc']\n",
    "        if short_location_name in short_label:\n",
    "            break\n",
    "                  \n",
    "    if short_label in peak_events:\n",
    "        chosen_date = peak_events[short_label]\n",
    "        level = mean_series.loc[chosen_date]\n",
    "        hand_params.append(\n",
    "            {\n",
    "                \"image\": f\"{short_location_name}\",\n",
    "                \"date\": f\"{chosen_date:%Y-%m-%d}\",\n",
    "                \"level\": f\"{level:0.2f}\"\n",
    "            }\n",
    "        )\n",
    "        # print(f'{short_label} \"{chosen_date}\" \"{level}\"')\n",
    "    else:\n",
    "        chosen_date = mean_series.index[mean_series.argmax()] \n",
    "        level = mean_series.loc[chosen_date]\n",
    "        chosen_date = chosen_date+ pandas.DateOffset(days=1)\n",
    "        # print(f'*{short_label} \"{chosen_date}\" \"{level}\"')\n",
    "        hand_params.append(\n",
    "            {\n",
    "                \"image\": f\"{short_location_name}\",\n",
    "                \"date\": f\"{chosen_date:%Y-%m-%d}\",\n",
    "                \"level\": f\"{level:0.2f}\"\n",
    "            }\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(hand_params).sort_values([\"image\",\"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas.DataFrame(hand_params).sort_values([\"image\", \"date\"]).to_csv(\n",
    "    f\"{os.environ['INPUT_FOLDER']}{os.path.sep}hand-levels.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c4f0e51b7554e248cceb5c2b9173e48e1ec03b89cb2754c7ccf7e560e9a5cb6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('my_v3_geo_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
